# Genome-Wide Association Studies {#asso}

This chapter focuses on Genome-Wide Associations Studies. It aims at explaining the principles and limitations of such studies. Section \@ref(genoquality) exposes the critical points to consider in terms of genotyping quality control to avoid false positives. Section \@ref(OR) introduces the concepts of disease penetrances and odds ratio generally used in genetic epidemiology. Section \@ref(popstructure) places emphasis on the problem of population structure in GWAS. In section \@ref(SMA) is explained the classical single marker approach used in GWAS while Section \@ref(multiloc) focuses on multi-marker methods to which we will refer in Chapter \@ref(LEOS) and \@ref(sicomore). 

## Introduction

Linkage analysis (Section \@ref(linkage) was the traditional approach
for disease gene mapping, where the co-segregation of marker alleles
with disease within large pedigrees or smaller family is studied. This
approach is efficient for locating genes contributing to simple
Mendelian disorders where there is a strong relationship between
phenotype and genotypes at the underlying functional polymorphisms.
However, it proved to be less reliable regarding mapping of complex
diseases as there may be multiple interacting genes underlying these
phenotypes and that the effects of these genes may vary according to
exposure to environmental and other non-genetic risk factors.

Whole Genome Association studies (WGA) focus on identifying genetic
markers that occur with different frequencies between samples of
unrelated affected individuals and unaffected controls, exploiting the
fact that it is easier to establish large cohorts of affected
individuals sharing a genetic risk factor for a complex disease across
the whole population than within individual families, as it is required
for traditional linkage analysis. WGA rely in two types of association
study: *direct association* and *indirect association*. On one hand,
direct association focus on directly genotyping and studying functional
polymorphisms which have relatively high prior probability of functional
relevance such as non-synonymous polymorphisms[^6], splice-site
variants[^7], and copy number polymorphisms (CNP[^8]). One the other
hand, indirect association, also referred as Genome-Wide Association
Study (GWAS), focuses on both functional SNP, such as non-synonymous
SNP, and those flanking them. Even if the flanking SNP are themselves
unlikely to be directly associated with the phenotype, at sufficiently
high density one or more is likely to be correlated (i.e. in linkage
disequilibrium, see Section \@ref(LD) with the underlying causal
variants.

Furthermore, recent breakthroughs in micro-array technology have meant
that hundreds of thousands of SNP can now be densely genotyped at
moderate cost. As a result, it has become possible to characterize the
genome of an individual with up to a million genetic markers. These
rapid advances in DNA sequencing technologies have also made it possible
to carry out exome and whole-genome sequencing studies of complex
diseases. In this context, Genome-Wide Association Studies have been
widely used to identify causal genomic variants[^9] implied in the
expression of different human diseases (rare, Mendelian or
multifactorial diseases). Thanks to the Next Generation Sequencing
techniques, it is now possible to genotype the complete DNA sequence of
an individual at a moderate cost, around 1000 \$ in 2016 [@NGS], and in
a very short time. Consequently, it is reasonable to think that the SNP
will be abandoned in favour of a complete genotype and it is therefore
necessary to develop statistical methods that can handle this kind of
massive data.

## Genotype quality control {#genoquality}

In GWAS, the data filtering step used to identify genotyping mistakes is
of primary importance since it can determine whether real discoveries
are made or just false positives wrongly interpreted. With such large
numbers of SNP being studied at the same time and with relatively
moderate sample sizes, even small genotyping error rates can have a
significant impact on the results.

As stated in [@wright2001complex], genetic effects on most
multifactorial phenotypes follow an L-shaped distribution, with a few
alleles having large effects and many alleles with a small effect size.
This means that GWAS principally aim to identify small differences in
allele frequencies between case and control, therefore even small
experimental error can have strong effects on the results, particularly
in the presence of rare alleles
[@clayton2005population; @barrett2006evaluating]. The following
paragraphs describe some of the filtering procedures designed to
identify issues on specific SNP.

### Deviation from HWE.

Neutral genetic variants in a large random-mating population are
expected to display Hardy–Weinberg Equilibrium (see Section
\@ref(HWE)). However, observed frequencies might be modified by
genotyping error, leading to a deviation from HWE. A traditional
approach for detecting genotyping errors is to test such deviation using
the Pearson goodness-of-fit statistic (Section \@ref(chi2) and to
look for significant deviations from the HWE [@weir1996genetic]. We
usually perform this test only in the control sample since a deviation
from HWE may also indicate an association with the disease. This test is
insensitive to small deviations that are most often observed and, in a
setting where there is a huge number of SNP to test for HWE deviation,
an appropriate threshold of significance is therefore difficult to
determine. Taking in account these considerations, the most prudent use
of HWE tests for genotyping error may be only to exclude the most
important deviations by setting an extreme significance threshold such
as $1.10^{-7}$ or less, and using exact tests for rare alleles
[@weir2005measures].

### Missing data.

In case-control studies, markers having large differences in missing
data rates between cases and controls often yield false positives
[@clayton2005population]. One can use the normal approximation to the
binomial distribution to test for significant differences in missing
data rates between cases and controls:
$$z = \frac{m_c - m_t}{\sqrt{m(1-m)(1/n_0+1/n_1)}},$$ with $m_c$ and
$m_t$ the proportion of missing genotypes among cases and controls
respectively, $n_0$ and $n_1$ the samples sizes of missing and
non-missing data and $m$ the overall missing genotype rate at the
marker.

### Distribution of test statistics.

When there are many significant loci coming out of a particular study it
may more likely reflect systematic genotype error in some of those
markers than reflect real discoveries. Indeed, remembering the L-shaped
distribution of effect sizes, a study with $10^5-10^6$ genetic markers
genotyped on one or two thousand cases and equal numbers of controls
should reveal few genuine loci with single locus *p*-values below
$1.10^{-6}$ [@zondervan2004complex]. Quantile-Quantile plot (Q-Q plot)
is an efficient graphical way to examine the distribution of *p*-value
and to evaluate whether there are too many data points in the tail. Q-Q
plots are constructed by ordering test statistics and plotting them
against the corresponding ordered expected values (see Figure
\@ref(fig:qqplot) for an example).

(ref:qqplot) Example of Quantile-Quantile plot (Q-Q plot) representing the distribution of the test statistic for a classical GWAS study (results from GWAS analysis on Bipolar disorder data coming from the Welcome Trust Case-Control Consortium [@burton_genome-wide_2007]). In this example we can see that the smallest *p*-value is equal to $4.5.10^{-5}$ and there are relatively few data points in the tail. Thus, based solely on the study of this distribution, there is therefore no reason to suspect genotyping errors.

```{r qqplot, echo=FALSE, fig.cap='(ref:qqplot)', out.width='70%',fig.asp=.75, fig.align='center'}
path <- "/Users/fguinot/Documents/bioptilamme/Manuscript/figures/qqplot_example.png"
img <-  grid::rasterGrob(as.raster(png::readPNG(path)), interpolate = FALSE)
gridExtra::grid.arrange(img, ncol=1)
```

## Disease penetrance and odds ratio {#OR}

Considering a biallelic locus with alleles $A$ and $a$, the possible
genotypes are then $A/A $, $A/a$ and $a/a$. The *disease penetrance*
associated with a given genotype is the risk of disease in individuals
carrying this genotype. Assuming a genetic penetrance parameter
$\gamma > 1$, the main disease penetrance models in association genetics
can be summarized as:

-   Multiplicative model: The risk of disease is increased by a factor
    of $\gamma$ with each additional $a$ allele

-   Additive model: The risk of disease is increased by a factor of
    $\gamma$ for genotype $A/a$ and by a factor of $2 \gamma$ for
    genotype $a/a$.

-   Recessive model: The risk of disease is increased by a factor of
    $\gamma$ for genotype $a/a$ only.

-   Dominant model: The risk of disease is increased by a factor of
    $\gamma$ both for genotype $A/a$ and $a/a$.

A commonly used measure of the strength of an association between
phenotype and genotype is the *relative risk* (RR), which compares the
disease penetrances between individuals carrying different genotypes
(Table \@ref(tab:penetrancetab)).

```{r penetrancetabfig, echo=FALSE, fig.align="center", out.width='70%', fig.asp=.5}
path <- "/Users/fguinot/Documents/bioptilamme/Manuscript/figures/penetrance_tab.png"
img <-  grid::rasterGrob(as.raster(png::readPNG(path)), interpolate = FALSE)
gridExtra::grid.arrange(img, nrow=1)
```
```{r penetrancetab, echo=FALSE, tidy=FALSE}
tab.cap <- c("Disease penetrances for genotype $A/A$, $A/a$ and $a/a$ and the associated relative risks for genotypes $A/a$ and $a/a$ with $f_0$ the disease penetrance of baseline genotype $A/A$ and $\\gamma$ the genetic penetrance parameter.")
knitr::kable(c(), caption = tab.cap, booktabs=TRUE, format="html")
```

To estimate the RR it is therefore necessary to assess the disease
penetrances which can only be derived directly from prospective cohort
studies. In these studies, a group of exposed and unexposed individuals
from the same population are followed up to evaluate who develop the
disease of interest. However, in a case-control study, in which the
case-control ratio is controlled by the investigator, it is not possible
to make direct estimates of disease penetrance, and hence of RRs. In
this type of study, the strength of an association is measured by the
*odds ratio* (OR) [@clarke2011basic].

In a case-control study, the odds of disease are defined as the
probability that the disease is present compared with the probability
that it is absent in exposed versus non-exposed individuals. Because of
selected sampling, odds of disease are not directly measurable. However,
conveniently, the disease OR is mathematically equivalent to the
exposure OR (the odds of exposure in cases versus controls), which can
be calculated directly from exposure frequencies [@balding2008handbook].
Two types of OR can be calculated:

-   Allelic OR: It is estimated by comparing the odds of disease in an
    individual carrying allele $A$ to the odds of disease in an
    individual carrying allele $a$.

-   Genotypic OR: It represents the association between disease and
    genotype by comparing the odds of disease in an individual carrying
    one genotype to the odds of disease in an individual carrying
    another genotype.

The risk factor for case versus control status is the genotype or allele
at a specific marker. For each SNP with minor allele $a$ and major
allele $A$ in case and control groups comprising $n$ individuals, it is
possible to represent the data as a $2 \times k$ contingency table of
disease status by either allele ($k = 2$) or genotype ($k = 3$) count
(Table \@ref(tab:contingencytab)).

```{r contingencytabfig, echo=FALSE, fig.align="center", out.width='90%', fig.asp=.35}
path <- "/Users/fguinot/Documents/bioptilamme/Manuscript/figures/contingency_tab.png"
img <-  grid::rasterGrob(as.raster(png::readPNG(path)), interpolate = FALSE)
gridExtra::grid.arrange(img, ncol=1)
```
```{r contingencytab, echo=FALSE, tidy=FALSE}
tab.cap <- c("$2 \\times 3$ contingency table of genotype counts and $2 \\times 2$ contingency table of allelic counts for a single locus with alleles $A$ and $a$. The genotype count $n_{ij}$ corresponds to the observed frequency of individuals carrying $i$ copies of the minor allele $a$ with phenotype $j=1$ for cases and $j=0$ for controls. The allelic count $m_{ij}$ can be summarized in different ways according to the disease penetrance models: for the dominant model, $i=0$ if an individual is $A/A$ and $i=1$ otherwise, for a recessive model $i=0$ if an individual is $A/A$ or $A/a$ and $i=1$ otherwise.")
knitr::kable(c(), caption = tab.cap, booktabs=TRUE, format="html")
```

Using the genotype count and allelic count exposed in Table
\@ref(tab:contingencytab), we define the allelic odds ratio ($OR_A$), the
allelic relative risk ($RR_A$) as:

$$OR_A = \frac{m_{01}m_{10}}{m_{00}m_{11}}, \hspace{10pt} RR_A = \frac{OR_A}{1 - p_0 + p_0 OR_A},$$
with $p_0$ is the estimated disease prevalence.

The genotypic odds ratio for genotype $a/a$ relative to genotype $A/A$
and for genotype $A/a$ relative to genotype $A/A$ is estimated by:
$$OR_{aa} =  \frac{n_{21}n_{00}}{n_{01}n_{20}}, \hspace{10pt} OR_{Aa} = \frac{n_{11}n_{00}}{n_{01}n_{10}}.$$

Given a disease prevalence $p_0$, the relative risk of disease in
individuals carrying a genotype $a/a$ compared with an $A/A$ genotype
is: $$RR_{AA} = \frac{OR_{AA}}{1-p_0 + p_0 OR_{AA}}.$$

Figure \@ref(fig:penetrancefig) illustrates the relationship between allele
frequency and disease penetrance in terms of disease representation.
Low-frequency alleles which also have a low penetrance are very
difficult to identify with common approaches while high-frequency
alleles are those most commonly identified.

(ref:penetrancefig) Relationship between allele frequency and penetrance on disease representation.

```{r penetrancefig, echo=FALSE, fig.cap='(ref:penetrancefig)', out.width='80%',fig.asp=.55, fig.align='center'}
path <- "/Users/fguinot/Documents/bioptilamme/Manuscript/figures/penetrance.jpg"
img <-  grid::rasterGrob(as.raster(jpeg::readJPEG(path)), interpolate = FALSE)
gridExtra::grid.arrange(img, ncol=1)
```

## Single Marker Analysis {#SMA}


The standard statistical method to identify variants associated with a
disease is to test the effect of each SNP one at a time using standard
hypothesis testing methods. The goal is to identify genetic variants
statistically associated with the phenotype, these variants being
themselves in linkage disequilibrium with a potential causal
polymorphism. Here we will review some of the most commonly used tests.

### Pearson’s $\chi^2$ statistic

The expected value under the independence hypothesis of the genotype
count $n_{ij}$, as defined in Table \@ref(tab:contingencytab), is noted as:

$$\mathbb{E}(n_{ij}) = \frac{n_{i.} n_{.j}}{N}.$$

It is thus possible to construct a genotypic association test by testing
the independence between the rows and columns of the contingency table
using the standard Pearson’s $\chi^2$ statistic for independence given
by:

$$\chi^2_{genotypic} = \sum_{i=[0,1,2]} \sum_{j = [0,1]}  \frac{(n_{ij} - \mathbb{E}(n_{ij}))^2}{\mathbb{E}(n_{ij})}.$$

This genotypic association test statistic has an approximate $\chi^2$
distribution with 2 degrees of freedom (d.f.) under the null hypothesis
$H_0$ of independence between the rows and columns of the contingency
table.

As shown in Table \@ref(tab:penetrancetab), it is also possible to consider
alternative models of penetrance by focusing on allele count rather than
genotype count. In this situation the allelic association test is
performed using a $2 \times 2$ contingency table and its associated
$\chi^2$ statistic is defined as:

$$\chi^2_{allelic} = \sum_{i=[0,1]} \sum_{j = [0,1]}  \frac{(m_{ij} - \mathbb{E}(m_{ij}))^2}{\mathbb{E}(m_{ij})}.$$

This allelic association test, which have 1 d.f., will be more powerful
than the genotypic test with 2 d.f., as long as the penetrance of the
heterozygote genotype is intermediate compared to those of the two
homozygous genotypes [@clarke2011basic].

### Cochran-Armitage trend test

Any penetrance model specifying a trend in risk with increasing numbers
of alleles $a$ can be examined using the Cochran-Armitage trend test
[@cochran1954some; @armitage1955tests] given by:
$$\chi^2_{CA} = \dfrac{ \left[ \sum_{i=0}^2 w_i(n_{.1}n_{2.} - n_{.2}n_{1.}) \right]^2}{\frac{n_{1.}n_{2.}}{n} \left[ \sum_{i=0}^2 w_i^2n_{.i}(n-n_{.i}) - 2\sum_{j=0}^1\sum_{i=j+1}^2 w_j w_i n_{.j} n_{.i} \right]},
\label{eq:CAtrend}$$ where $w = (w_0,w_1,w_2)$ are weights chosen to
detect particular types of association. For instance, with a dominant
model \( w = (0,1,1) \) is optimal while for a recessive model weights
$ w=(0,0,1) $ are rather chosen.

Under the null hypothesis of no association between the SNP and disease
($H_0:$ independence between rows and columns of the contingency table),
$\chi^2_{CA}$ has an approximate $\chi^2$ distribution with 1 d.f.. The
power of this test is often improved as long as the disease risks
associated with the $A/a$ genotype are intermediate to those associated
with the $a/a$ and $A/A$ genotypes. In GWAS, in which the underlying
genetic model is unknown, the additive version of this test, i.e. with
$w = (0,1,2)$, is most commonly used.

### Logistic regression and likelihood ratio test {#logitGWAS}

Another possible framework for modelling the relationship between a
case-control phenotype and SNP genotype is to use the logistic
regression model, as described in Section \@ref(glm). The logistic
regression model is parametrised in terms of the log-odds of disease for
each SNP genotype, denoted by $\boldsymbol{\beta}$. The log-likelihood of observed
phenotype data, $\mathbf{y}$ and genotype data $\mathbf{G}$, is given by:

$$l(\mathbf{y} | \mathbf{G},\boldsymbol{\beta}) = \sum_{i=1}^n \left[ y_i \log \left( \frac{e^{\eta_i}}{1+e^{\eta_i}}\right) +  (1-y_i)\log \left( 1 - \frac{e^{\eta_i}}{1+e^{\eta_i}} \right) \right],$$
where the linear predictor $\eta_i = \beta_0 + \beta g_i$.

Under the null hypothesis of no association $H_0: \boldsymbol{\beta} = 0$ , we
expect each genotype to have equal odds of disease, so that
$\eta_i = \beta_0$. Under the additive model and treating allele $A$ as
baseline, the linear predictor becomes:

$$\eta_i = \beta_0 + \beta_A z_{(A)i},$$ where $\beta_A$ corresponds to
the additive effect of allele $a$ and $z_{(A)i}$ is a variable
representing the additive component of the $i$th genotype (see Table
\@ref(tab:SNPcode) for the SNP coding in different disease penetrance
models).

```{r SNPcodefig, echo=FALSE, out.width='75%',fig.asp=.32, fig.align='center'}
path <- "/Users/fguinot/Documents/bioptilamme/Manuscript/figures/SNPcode.png"
img <-  grid::rasterGrob(as.raster(png::readPNG(path)), interpolate = FALSE)
gridExtra::grid.arrange(img, ncol=1)
```
```{r SNPcode, echo=FALSE, tidy=FALSE}
tab.cap <- c("Coding of additive, dominance and recessive components of SNP genotypes.")
knitr::kable(c(), caption = tab.cap, booktabs=TRUE, format="html")
```

In this framework, tests of association can be conducted with likelihood
ratio (LR) methods in which inference is based on the likelihood of the
genotyped data given disease status. The likelihood of the observed data
under the proposed model of disease association is compared with the
likelihood of the observed data under the null model of no association.
For example, the log-likelihood ratio statistics,

$$\Lambda_{Gen} =  l(\mathbf{y} | \mathbf{G},\hat{\beta}_0,\hat{\beta}_A) - 2 l(\mathbf{y} | \mathbf{G},\hat{\beta}_0,\hat{\beta}_A = 0),$$
provides a genotype-based test of association which have an approximate
$\chi^2$ distribution with 2 d.f. under the null hypothesis. In large
samples, it can be shown that $\chi^2$ and LR methods are equivalent
under the null hypothesis [@rice2006mathematical].

Furthermore, by using the flexible logistic regression framework, it is
straightforward to incorporate additional covariates in the linear
component, to allow the modelisation of environmental effects or to
correct for population structure as we will see in the following
section. The linear predictor $\eta_i$ can thus be extend to:

$$\eta_i = \beta_0 + \sum_{j=1}^p \alpha_j x_{ij} + \beta_A z_{(A)i},$$
where $x_{ij}$ is the response of the $i^{th}$ individual to the
$j^{th}$ covariate and $\alpha_j$ its corresponding coefficient.
Covariate adjustment reduces spurious associations due to sampling
artefacts or biases in study design, but adjustment comes at the price
of using additional degrees of freedom which may impact statistical
power.

## Limitations {#GWASlimits}

The classical Single Marker Analysis approach is subject to false
positives (i.e. SNP that are falsely identified as significant
variables) due to the number of tests performed at the same time. One
way around this problem is to apply a correction for multiple
comparisons as described in Section \@ref(multiple). Unfortunately,
this increases the risk of missing true associations that have only a
small effect on the phenotype, which is usually the case in GWAS.
Indeed, simultaneously testing $1.10^5$ SNP with single marker analysis
would require that the associated *p*-value reach a threshold of at
least $5.10^{-5}$, using a Bonferroni correction, to be consider as
significant and a little higher with FDR control method.

Furthermore, another commonly used approach for multiple testing
comparisons in GWAS relies on the concept of genome-wide significance.
It is based on the distribution of LD in the genome for a specific
population and consider that there are an “effective” number of
independent genomic regions, and thus an effective number of statistical
tests that should be corrected for. For European-descent populations,
this threshold has been estimated at $7.2.10^{-8}$
[@dudbridge2008estimation]. This approach should however be used with
caution since the only scenario where this correction is appropriate is
when hypotheses are tested on the genome scale. Candidate gene studies
or replication studies with a focused hypothesis do not require
correction to this level, as the number of effective, independent
statistical tests is much lower than what is assumed for genome-wide
significance [@bush2012genome].

Furthermore, as stated in [@maher_personal_2008], these approaches face
other limitations:

-   It does not directly account for correlations among the predictors,
    whereas these correlations can be very strong as a result of linkage
    disequilibrium (LD). SNP can be correlated even where they are not
    physically linked, because of population structure or epistasis
    (gene by gene interactions).

-   It does not account for epistasis, i.e. causal effects that are only
    observed when certain combinations of mutations are present in the
    genome.

-   It does not directly provide predictive models for estimating the
    genetic risk of the disease.

-   It focuses on identifying common markers with minor
    allele frequency (MAF) above 5$\%$, although it is likely that
    analysing low-frequency ($0.5\% <$ MAF $< 5\%$) and rare (MAF $<0.5\%$) variants would be able to explain additional disease risks
    or trait variability [@lee_rare-variant_2014].

Uncovering some of the missing heritability can sometimes be achieved by
taking into account correlations among variables, interaction with the
environment, and epistasis, but this is rarely feasible in the context
of GWAS because of the multiple testing burden and the high
computational cost of such analyses [@manolio_finding_2009]. That is
why, knowing these limitations, we propose in Chapter 4 a
new approach that take benefit of the correlation structure among SNP to
improve statistical power in GWAS.

## Population structure {#popstructure}

One of the most important covariate to consider in GWAS is the measure
of population structure which, if not accounted for, can inflate the
false positive error rate. As stated in Section \@ref(#originLD), we
know that population stratification as an important impact on patterns
of LD and allele frequencies are highly variable across human
subpopulations, meaning that in a sample with multiple strata,
strata-specific SNP will likely be associated to the trait due to
population structure. As a result, SNP with allele frequency differences
between the strata will appear to be associated with disease, even if
there is no association within each stratum. Several methods to identify
and adjust for population stratification have been developed of which
the most commonly used are genomic control, structured association and
principle components correction [@balding2008handbook].

### Genomic control

Under the null hypothesis of no disease association, the distribution of
Cochran–Armitage test statistics is $\chi_{CA}^2$ with 1 d.f. However,
in a stratified population, we expect different allele frequency at many
SNP and hence an excess of false positive signals of association. As a
result, the observed distribution of association statistics will be
inflated by a genomic inflation factor $\lambda$ [@devlin1999genomic].
The genomic inflation factor \(\lambda\) is defined as the ratio of the
median of the empirically observed distribution of the test statistic to
the expected median: $$\lambda=\text{median}(\chi_{CA}^2)/0.456.$$

The genomic control method takes account of structure by a linear
rescaling of observed test statistics to approximately restore the
$\chi_{CA}^2$ with 1 d.f null distribution:
$$\chi^2_{CA-adj}=\chi_{CA}^2/\lambda.$$

### Structured association

The method known as structured association, implemented in the STRUCTURE
software [@pritchard2000inference], uses an admixture model[^10] where
the proportion of an individual’s genome into $K$ specific ancestral
strata is treated as unknown. The posterior distribution of ancestry for
each individual is then approximated using bayesian Markov Chain Monte
Carlo (MCMC) methods based on genotype information from several hundred
genome-wide SNP and the estimated structure is then included as
covariates in a logistic regression framework. The main drawback of this
approach is that the number of ancestral subpopulations must be inferred
using an ad hoc estimation procedure and the computational load of the
MCMC algorithm is such that it cannot accommodate for the numbers of
markers commonly used in GWAS.

### Principle components correction {#PCC}

This method makes use of the Principal Component Analysis (PCA) to
detect and correct for population structure. In PCA, the few first
principal components, calculated using the eigen-decomposition of a
matrix, explain the greatest amount of variation in the data and has
long been used to study population structure in genetic data
[@reich2008principal]. In GWAS, PCA has been used to explicitly model
ancestry differences between cases and controls along continuous axes of
variation and the first principle components may be used as covariates
in a logistic regression model to adjust for the population structure
effect. PCA being a computationally efficient algorithm, this approach
has the advantage that it can be applied to datasets with more than
$1.10^5$ SNP.

The software EIGENSTRAT [@price_principal_2006] use this approach by
computing an adjusted test statistic defined as follow:
$$\chi^2_{eigen} = (n-k-1) r^2(\mathbf{z}_m^{adj},\mathbf{y}^{adj}),$$ where
$\mathbf{z}_m^{adj}$ is the adjusted genotype at marker $m$, defined as the
residuals after regressing genotypes on the top $k$ principal
components. The adjusted phenotype $\mathbf{y}^{adj}$ is similarly defined. The
test statistic $\chi^2_{eigen}$ approximately follows a $\chi^2$
distribution with 1 d.f under the null hypothesis of no association. It
has been shown that the EIGENSTRAT method has a higher power than
genomic control because the correction in EIGENSTRAT is specific to a
variation in frequency of a candidate marker across ancestral
populations, which will minimize spurious associations as well as
maximize power to detect true associations [@price_principal_2006] .

## Multi-locus analysis {#multiloc}

As previously mentioned, in GWAS it is necessary for the SNP to be
correlated to the causal polymorphisms in order to have an efficient
disease mapping and, in complex disease, each single SNP have small
effects on the phenotype. In this section we will show that we can take
benefit from performing joint association tests of multiple SNP flanking
a causal polymorphism to increase power in the case of rare-variant
analysis or when the genetic effects are too small to be detected by
single-locus approaches.

Several ways of grouping SNP together for multi-locus analysis are
possible; we may consider to group SNP that fall within an established
biological context such as a biochemical pathway, protein family, or
gene. We can also consider working at haplotypes level rather than the
genotypes and used the haplotype structure of the genome to define
relevant groups (Section \@ref(rare-variant) and \@ref(adjclust)).

Performing multi-locus analysis is not as straightforward as single
marker analysis and presents some computational and statistical
challenges. The most commonly used model to regress multiple SNP is the
multiple linear regression with which we can simultaneously fit all SNP
in the same gene or small genomic region. To reduce the problem of
collinearity and overfitting that may arise, we can resort to penalized
approaches, as described in Section \@ref(penalized), such as ridge,
lasso or group-lasso regression models. Furthermore, with such models,
it is also possible to examine statistical interactions among genetic
variants and so to investigate epistatic effects as in
[@stanislas_eigen-epistasis_2016].

Other methods using multiple linear regression take into account the
linkage disequilibrium within the genes to improve power
[@yoo_multiple_2016] or cluster variants with weak association around
known loci to increase the percentage of variance explained in complex
traits [@pare_contribution_2015]. Finally, other approaches will focus
on the aggregation of summary statistics of single SNP within a same
gene with for instance the data-driven aggregation of summary statistics
described in [@kwak_adaptive_2016] or the procedures of $p$-value
combination in [@petersen_assessing_2013].

### Haplotype-based approaches

One approach to multi-locus analysis is to focus on haplotype effects.
As seen in Section \@ref(haplo), the human genome can be partitioned
into haplotype blocks where most of the intra-block variability is
imputable to mutation rather than recombination. As a result, much of
common genetic variation can also be structured into haplotypes within
blocks of LD that are rarely disturbed by meiosis.

It is common to assume that each of the pair of haplotypes, $H_{i1}$ and
$H_{i2}$, labelled according to their relative frequency in the
population and forming the diplotype $H_i$ of the $i^{th}$ individual,
contributes independently to the disease risk. Under this assumption the
logistic regression model can be parametrised in terms of the log odds
of disease for each haplotype [@balding2008handbook]. The linear
predictor $\eta_i$ of the $i^{th}$ individual, as defined in Section
\@ref(logitGWAS), can thus be defined as:

$$\eta_i = \beta_0 + \sum_{j=1}^p \alpha_j x_{ij} + \beta_{H_{i1}} + \beta_{H_{i2}},$$
where $x_{ij}$ is the response of the $i^{th}$ individual to the
$j^{th}$ covariate and $\alpha_j$ its corresponding coefficient.
$\beta_k$ denotes the log-OR of the $k^{th}$ most frequent haplotype,
relative to the baseline haplotype, usually the most common, so that
$\beta_1 = 0$.

One major issue with this approach is that we do not directly observe
the diplotype $H$ from the unphased genotype data. One solution is to
take a point estimate of the diplotype for each individual, using
statistical methodology, such as PHASE [@stephens2001new] or by maximum
likelihood using the expectation-maximisation algorithm
[@excoffier1995maximum]. However, due to the uncertainty in the
haplotype reconstruction process, the variances of the model parameters
are under-estimated leading to an inflation of type I error
[@balding2008handbook].

### Rare-variant association analysis {#rare-variant}

In the context of rare-variant association analysis, a number of region-
or gene-based multimarker tests have been proposed as burden
tests [@asimit_ariel_2012], variance-component
tests [@wu_rare-variant_2011] or combined burden and variance-component
tests [@lee_optimal_2012]. Instead of testing each variant individually,
these methods evaluate the cumulative effects of multiple genetic
variants in a gene or a region, increasing power when multiple variants
in the group are associated with a given disease or trait.

We first introduce the statistical model used in various rare-variant
tests and that is again based on the logistic regression framework
defined in Section \@ref(logitGWAS). We assume that $n$ individuals
have been genotyped in a region comprising $M$ genetic markers and
defined the linear predictor $\eta_i$ of the $i^{th}$ individual as:

$$\eta_i = \beta_0 + \sum_{j=1}^p \alpha_j x_{ij} + \sum_{m=1}^M \beta_m z_{im},
\label{eq:linearpred}$$

where $z_{im} = z_{(A)im}$ is the variable representing the additive
component of the $i^{th}$ individual for the $m^{th}$ marker and
$x_{ij}$ the response of the $i^{th}$ individual to the $j^{th}$
covariate. We define the score statistic of the model for variant $m$ as
$$S_m = \sum_{i=1}^n z_{im} (y_i - \eta_i).$$ Note that $S_m$ is
positive when marker $m$ is associated with increased disease risk or
trait values and negative when marker $m$ is associated with decreased
risk or trait values.

#### Burden tests {.unnumbered}

Burden tests [@asimit_ariel_2012; @li2008methods] compute a single
genetic score from multiple genetic markers and test for association
between this score and a phenotype of interest. A simple approach
summarizes genotype information by counting the number of minor alleles
across all variants in the set. The summary genetic score is then:
$$C_i = \sum_{m=1}^M \omega_m z_{im},
\label{eq:burden_fun}$$ where $w_m$ is a weight attributed to marker
$m$. The linear predictor can thus be written as:
$$\eta_i =  \beta_0 + \sum_{j=1}^p \alpha_j x_{ij} + \beta_1 C_i.$$

To compute a $\textit{p}$-value for a set of $M$ markers, the specific
test statistic $Q_{burden}$ is calculated and compared to a $\chi^2$
distribution with 1 d.f.:
$$Q_{burden} = \left[ \sum_{m=1}^M \omega_m S_m \right]^2.$$

This framework is flexible in the sense that we can attribute different
weights to the markers or define the genetic score $C_i$ to accommodate
for different assumptions about disease mechanism. For instance, the
cohort allelic sums test [CAST, @morgenthaler2007strategy] assumes that
the presence of any rare variant increases disease risk and sets the
genetic score $C_i = 0$ if there are no minor alleles in a region and
$C_i = 1$ otherwise. Furthermore, to focus on rarer variants, we can
assign $w_m = 1$ when the MAF of variant $m$ is smaller than a
prespecified threshold and $w_m = 0$ otherwise. Alternatively, a
continuous weight function can be used to upweight rare variants with
for instance $\omega_m = 1 / \sqrt{\text{MAF}_m (1 - \text{MAF}_m)}$ as
proposed by [@madsen2009groupwise].

The burden methods make a strong assumption that all rare variants in a
set are causal and associated with a trait with the same direction and
magnitude of effect (after adjustment for the weights) which may in a
substantial loss of power if these assumptions prove to be false
[@lee_rare-variant_2014].

#### Sequence Kernel Association Test (SKAT) {.unnumbered}

In [@wu_powerful_2010], the authors proposed to group SNP into sets on
the basis of their proximity to genomic features such as genes or
haplotype blocks and then to identify the joint effect of each set via a
logistic kernel-machine-based test. This approach lays the foundation
for the Sequence Kernel Association Test method [SKAT,
@wu_rare-variant_2011].

SKAT uses the same logistic regression framework and the linear
predictor as with burden tests but instead of testing the null
hypothesis $H_0: \beta_1, \dots, \beta_M = 0$, it assumes that each
$\beta_m$ follows an arbitrary distribution with a mean of zero and
variance of $\omega_m \tau$ where $\tau$ is a variance component and
$\omega_m$ the weight attributed to marker $m$. With this assumption, we
can see that $H_0: \beta_1, \dots, \beta_M = 0$ is equivalent to test
$H_0: \tau = 0$ which can be efficiently tested with a
variance-component score test as used in generalized linear mixed model
(GLMM) and is known to be a locally most powerful test
[@lin1997variance]. An advantage of this score test is that it requires
to fit only the null model and to compute the following
variance-component score statistic:
$$Q_{SKAT} = \sum_{i=1}^n \sum_{i'=1}^n (y_i - \eta_i) K(\mathbf{z}_i,\mathbf{z}_{i'}) (y_{i'} - \eta_{i'})$$
where $\eta_i =  \beta_0 + \sum_{j=1}^p \alpha_j x_{ij}$ is the linear
predictor of the null model including only the $p$ covariates for
individual $i$, and where
$K(\mathbf{z}_{i},\mathbf{z}_{i'}) = \sum_{m=1}^M \omega_m z_{im} z_{i'm}.K(.,.)$
is called the kernel function and measures the genetic similarity
between individuals $i$ and $i'$, weighted by a factor $\omega_m$, via
the $M$ genetic markers in the region of interest. This particular form
of $K(.,.)$ is
called the weighted linear kernel function and can take several forms to
accommodate for epistatic effects for instance. In fact, any positive
semi-definite function can be used as a kernel function and in their
paper, [@wu_rare-variant_2011] tailored the following commonly used
kernels specifically for the purpose of rare-variant analysis:

-   The weighted linear kernel:
    $$K(\mathbf{z}_{i},\mathbf{z}_{i'}) = \sum_{m=1}^M \omega_m z_{im} z_{i'm}$$
    implies a linear relationship between the trait of interest and the
    genetic variants and is equivalent to the classical linear and
    logistic model described in Section \@ref(logitGWAS).

-   The weighted quadratic kernel:
    $$K(\mathbf{z}_{i},\mathbf{z}_{i'}) = (1 + \sum_{m=1}^M \omega_m z_{im} z_{i'm})^2$$
    assumes that the model depends on the main effects and quadratic
    terms for the gene variants and the first-order variant by variant
    interactions.

-   The weighted identity by state (IBS[^11]) kernel:
    $$K(\mathbf{z}_{i},\mathbf{z}_{i'}) = \sum_{m=1}^M \omega_m IBS(z_{im},z_{i'm})$$
    defines similarity between individuals as the number of alleles that
    share IBS.

Under the null hypothesis, $Q_{SKAT}$ follows a mixture of chi-square
distributions, which can be closely approximated with the
computationally efficient Davies method [@davies1980algorithm].

### LD based approach to variable selection in GWAS {#adjclust}

Region-based multi-marker analysis necessarily need that we define a
group structure among SNP either by using the gene definition or
biochemical pathway. However, these approaches limit the search for
association to coding region only and therefore potential interesting
associations located in non-coding region[^12] are set aside.

One way to circumvent this issue is to use non-supervised clustering
techniques such as hierarchical clustering described in Section
\@ref(CAH). In their paper, [@dehman_performance_2015] proposed an
approach where they used a modified version of the hierarchical
clustering combined with a group-lasso regression to select groups of
markers associated with phenotype of interest. The clustering method
used is a spatially constrained agglomerative hierarchical clustering
based on Ward’s criterion in which the measure of dissimilarity is not
based on the Euclidean distance but rather on the linkage disequilibrium
level between two markers: $1 - r^2(m,m')$. The algorithm also makes use
of the fact that the LD matrix can be modelled as block-diagonal by
allowing only groups of variables that are adjacent on the genome to be
merged, which significantly reduces the computation cost [*adjclust*,
@dehman:tel-01288568].

The number of groups is then determined using a modified version of the
gap statistic defined in Section \@ref(CAH):

$$Gap(g) = \frac{1}{B} \sum_{b=1}^B(I_{W_g}^b - I_{W_g}),$$ where for
$b = 1,\dots,B$, $I_{W_g}^b$ denotes the within-cluster dispersion of
clustering the reference dataset $b$ in $g$ groups. They decided to use
the $I_{W_g}$ instead of $\log(I_{W_g})$ in estimation since they
noticed that it led to better estimation of the number of groups in the
simulation studies, which were performed under a variety of parameters
and on several data sets.

Finally, once the LD-defined groups structure have been determined, a
group-lasso regression is performed in order to select groups of SNP
associated with the phenotype. Given a phenotype vector $\mathbf{y}$ and a
scaled matrix $\mathbf{Z}$ of additively coded SNP, the group-lasso estimate is
defined as:

$$\hat{\beta}^{GL} = \underset{\boldsymbol{\beta}}{\text{argmin}} \left[ ||\mathbf{y} - \mathbf{Z}\boldsymbol{\beta}||^2_2 + \lambda \sum_{g=1}^G \sqrt{p_g}||\boldsymbol{\beta}^g||_2  \right],$$
where $||.||$ denotes the euclidean norm, $\lambda > 0$ is a penalty
factor and $\boldsymbol{\beta}_g$ the vector of regression coefficients
corresponding to the $g^{th}$ group, so that
$\boldsymbol{\beta} = (\boldsymbol{\beta}^1,\dots,\boldsymbol{\beta}^G)$.

[^6]: A non-synonymous SNP is a SNP that modifies the protein sequence
    in opposition to a synonymous SNP.

[^7]: A genetic alteration in the DNA sequence that occurs at the
    boundary of an exon and an intron (splice site). This change can
    disrupt RNA splicing, resulting in the loss of exons or the
    inclusion of introns leading to an altered protein-coding sequence.

[^8]: A CNP is a normal variation in DNA due to the varying number of
    copies of a sequence within the DNA. Large-scale copy number
    polymorphisms are common and widely distributed throughout the
    genome.

[^9]: In the remainder of the paper, the terms variant, marker, locus,
    SNP or polymorphism will equivalently refer to the variable studied
    in GWAS.

[^10]: An admixture model is a statistical model taking in account the
    phenomenon known as population admixture (see Section
    \@ref(#originLD)).

[^11]: A DNA segment is identical by state (IBS) in two or more
    individuals if they have identical nucleotide sequences in this
    segment.

[^12]: Non-coding DNA (formerly referred as ’junk’ DNA) represents 99$\%$
    of the genome and does not provide instructions for making proteins.
    However, recent studies have shown that non-coding DNA sequences can
    act as regulatory elements like sites for transcription factors
    implied in the control of gene transcription (source:
    <https://ghr.nlm.nih.gov/primer/basics/noncodingdna>).
